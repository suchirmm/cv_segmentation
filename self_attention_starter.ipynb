{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699ebf9-f68c-4e5b-9b6b-63b60c0dcc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1: Setup and Configuration\n",
    "#\n",
    "# First, let's define some configuration parameters similar to what a Llama 4 model might use and create some sample input data.\n",
    "\n",
    "# %%\n",
    "# Configuration (Simplified for clarity)\n",
    "hidden_size = 128  # Dimensionality of the model's hidden states\n",
    "num_attention_heads = 16 # Total number of query heads\n",
    "num_key_value_heads = 4  # Number of key/value heads (for GQA)\n",
    "head_dim = hidden_size // num_attention_heads # Dimension of each attention head\n",
    "max_position_embeddings = 256 # Maximum sequence length the model expects\n",
    "rope_theta = 10000.0 # Base for RoPE frequency calculation\n",
    "rms_norm_eps = 1e-5 # Epsilon for RMSNorm\n",
    "attention_bias = False # Whether to use bias in Q \n",
    "attention_dropout = 0.0 # Dropout probability for attention weights\n",
    "use_qk_norm = True # Whether to apply L2 norm to Q and K before attention\n",
    "\n",
    "# Sample Input\n",
    "batch_size = 2\n",
    "sequence_length = 10\n",
    "hidden_states = torch.randn(batch_size, sequence_length, hidden_size)\n",
    "# Create position IDs for each token in the sequence, repeated for each batch\n",
    "# torch.arange(0, sequence_length) generates a 1D tensor with values from 0 to sequence_length-1\n",
    "# The unsqueeze(0) adds an extra dimension at the 0th position, making it (1, sequence_length)\n",
    "# This allows repeat(batch_size, 1) to create a tensor of shape (batch_size, sequence_length)\n",
    "position_ids = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1) # Shape: (batch_size, sequence_length)\n",
    "# Simple causal mask (upper triangular) for demonstration\n",
    "# In reality, Llama4 uses a more complex mask creation including padding handling\n",
    "attention_mask = torch.triu(torch.ones(sequence_length, sequence_length) * -torch.inf, diagonal=1)\n",
    "attention_mask = attention_mask.unsqueeze(0).unsqueeze(0) # Shape: (1, 1, sequence_length, sequence_length)\n",
    "attention_mask = attention_mask.expand(batch_size, 1, -1, -1) # Shape: (batch_size, 1, sequence_length, sequence_length)\n",
    "\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  hidden_size: {hidden_size}\")\n",
    "print(f\"  num_attention_heads: {num_attention_heads}\")\n",
    "print(f\"  num_key_value_heads: {num_key_value_heads}\")\n",
    "print(f\"  head_dim: {head_dim}\")\n",
    "\n",
    "print(\"\\nSample Input Shapes:\")\n",
    "print(f\"  hidden_states: {hidden_states.shape}\")\n",
    "print(f\"  position_ids: {position_ids.shape}\")\n",
    "print(f\"  attention_mask: {attention_mask.shape}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2: Q, K, V Projections\n",
    "#\n",
    "# The first step in attention is to project the input `hidden_states` into Query (Q), Key (K), and Value (V) representations using linear layers.\n",
    "#\n",
    "# - **Q:** Represents the current token's query.\n",
    "# - **K:** Represents the keys of all tokens in the sequence (or context).\n",
    "# - **V:** Represents the values (information) of all tokens.\n",
    "#\n",
    "# Llama 4 uses Grouped-Query Attention (GQA). This means there are fewer K and V heads than Q heads. The `num_key_value_groups` tells us how many Q heads share the same K and V head. This reduces computation and memory requirements.\n",
    "\n",
    "# %%\n",
    "# Define projection layers\n",
    "q_proj = nn.Linear(hidden_size, num_attention_heads * head_dim, bias=attention_bias)\n",
    "k_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=attention_bias)\n",
    "v_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=attention_bias)\n",
    "o_proj = nn.Linear(num_attention_heads * head_dim, hidden_size, bias=attention_bias)\n",
    "\n",
    "# Calculate projections\n",
    "query_states = q_proj(hidden_states)\n",
    "key_states = k_proj(hidden_states)\n",
    "value_states = v_proj(hidden_states)\n",
    "\n",
    "# Reshape Q, K, V for multi-head attention\n",
    "# Target shape: (batch_size, num_heads, sequence_length, head_dim)\n",
    "query_states = query_states.view(batch_size, sequence_length, num_attention_heads, head_dim).transpose(1, 2)\n",
    "key_states = key_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "value_states = value_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "\n",
    "print(\"Projected Shapes:\")\n",
    "print(f\"  query_states: {query_states.shape}\") # (batch_size, num_attention_heads, sequence_length, head_dim)\n",
    "print(f\"  key_states: {key_states.shape}\")     # (batch_size, num_key_value_heads, sequence_length, head_dim)\n",
    "print(f\"  value_states: {value_states.shape}\")   # (batch_size, num_key_value_heads, sequence_length, head_dim)\n",
    "\n",
    "num_key_value_groups = num_attention_heads // num_key_value_heads\n",
    "print(f\"\\nNum Key/Value Groups (Q heads per K/V head): {num_key_value_groups}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3: Rotary Positional Embeddings (RoPE)\n",
    "#\n",
    "# Instead of adding absolute positional embeddings, Llama models use Rotary Positional Embeddings (RoPE). RoPE applies rotations to the Q and K vectors based on their position, injecting relative positional information directly into the query and key representations *before* the dot product. This is often found to improve performance, especially on long sequences.\n",
    "#\n",
    "# The core idea is to represent the embeddings in complex number space and rotate them by an angle proportional to their position.\n",
    "#\n",
    "# Llama 4 conditionally applies RoPE (`use_rope` in the original code, often skipping it for certain layers in large models to potentially handle very long contexts differently). For this example, we'll assume it's applied.\n",
    "#\n",
    "# The `Llama4TextRotaryEmbedding` class calculates the complex frequencies `freqs_cis` based on `position_ids`. The `apply_rotary_emb` function then applies these rotations to Q and K.\n",
    "\n",
    "# %%\n",
    "# Simplified RoPE Calculation and Application (Illustrative)\n",
    "\n",
    "def simple_rope_calculation(dim, max_seq_len, base=10000.0, device=None):\n",
    "    \"\"\"Calculates simplified RoPE frequencies.\"\"\"\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(max_seq_len, device=device).type_as(inv_freq)\n",
    "    freqs = new_func(inv_freq, t)\n",
    "    # Different from paper, but aligns with HF implementation:\n",
    "    # freqs = torch.cat((freqs, freqs), dim=-1) # Shape: (max_seq_len, dim)\n",
    "    # freqs_cis = torch.polar(torch.ones_like(freqs), freqs) # complex64\n",
    "    # Alternative way to get complex numbers:\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)\n",
    "    # Calculate cosine and sine of the embeddings\n",
    "    # Cosine and sine functions are used to represent the real and imaginary parts of a complex number.\n",
    "    # Intuitively, this allows us to encode rotational transformations in a 2D plane, which is crucial for\n",
    "    # capturing relative positional information in the sequence.\n",
    "    # Mathematically, the cosine function provides the real part of the complex exponential representation,\n",
    "    # while the sine function provides the imaginary part. This is based on Euler's formula: e^(ix) = cos(x) + i*sin(x).\n",
    "    freqs_cos = emb.cos() # Real part\n",
    "    freqs_sin = emb.sin() # Imaginary part\n",
    "    # Combine the real and imaginary parts to form complex numbers\n",
    "    # This complex representation allows for efficient rotation of vectors, which is key in Rotary Positional Embeddings (RoPE).\n",
    "    freqs_cis = torch.complex(freqs_cos, freqs_sin) # Shape: (max_seq_len, dim)\n",
    "    return freqs_cis\n",
    "\n",
    "def new_func(inv_freq, t):\n",
    "    freqs = torch.outer(t, inv_freq)\n",
    "    return freqs\n",
    "\n",
    "def apply_rotary_emb_torch(\n",
    "    xq: torch.Tensor,      # Query tensor, shape (batch, num_heads, seq_len, head_dim)\n",
    "    xk: torch.Tensor,      # Key tensor, shape (batch, num_heads, seq_len, head_dim) - Simplified assumption\n",
    "    freqs_cis: torch.Tensor, # Precomputed complex rotations, shape (max_seq_len, head_dim)\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Applies RoPE rotations to Q and K using torch complex numbers.\"\"\"\n",
    "\n",
    "    # 1. Ensure freqs_cis is on the right device (CPU/GPU)\n",
    "    freqs_cis = freqs_cis.to(xq.device)\n",
    "\n",
    "    # 2. Select the correct rotation vectors for the current sequence positions\n",
    "    #    position_ids has shape (batch, seq_len)\n",
    "    #    This uses advanced indexing to pick rows from freqs_cis based on position_ids\n",
    "    freqs_cis = freqs_cis[position_ids] # Now shape: (batch, seq_len, head_dim), complex\n",
    "\n",
    "    # 3. Add a dimension for broadcasting across attention heads\n",
    "    #    We want the same rotation applied to all heads for a given token/position\n",
    "    freqs_cis = freqs_cis[:, None, :, :] # Now shape: (batch, 1, seq_len, head_dim), complex\n",
    "\n",
    "    # --- Prepare Q and K for Complex Multiplication ---\n",
    "\n",
    "    # 4. Reshape Q and K to view adjacent pairs as complex numbers\n",
    "    #    xq: (batch, num_heads, seq_len, head_dim)\n",
    "    #        -> reshape to (batch, num_heads, seq_len, head_dim // 2, 2)\n",
    "    #        -> view as complex -> (batch, num_heads, seq_len, head_dim // 2), complex\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "\n",
    "    # --- Prepare freqs_cis for Complex Multiplication ---\n",
    "\n",
    "    # 5. Select the necessary part of freqs_cis for complex math\n",
    "    #    The original freqs_cis had duplicated angles. We only need the first half\n",
    "    #    (corresponding to head_dim // 2 unique frequencies) for complex multiplication.\n",
    "    #    Input freqs_cis shape: (batch, 1, seq_len, head_dim), complex\n",
    "    #    Output shape: (batch, 1, seq_len, head_dim // 2), complex\n",
    "    freqs_cis_broadcast = freqs_cis[..., :xq_.shape[-1]] # Slices the last dim\n",
    "\n",
    "    # --- Apply the Rotation ---\n",
    "\n",
    "    # 6. Perform the RoPE rotation using element-wise complex multiplication\n",
    "    #    xq_ (batch, num_heads, seq_len, head_dim / 2) *\n",
    "    #    freqs_cis_broadcast (batch, 1, seq_len, head_dim / 2)\n",
    "    #    The division by 2 is because we are treating pairs of values as complex numbers.\n",
    "    #    The '1' in freqs_cis_broadcast broadcasts across the 'num_heads' dimension.\n",
    "    rotated_xq = xq_ * freqs_cis_broadcast\n",
    "    rotated_xk = xk_ * freqs_cis_broadcast\n",
    "\n",
    "    # --- Convert Back to Real Representation ---\n",
    "\n",
    "    # 7. Convert the rotated complex vectors back to real vectors\n",
    "    #    rotated_xq (..., head_dim // 2) complex\n",
    "    #        -> view_as_real -> (..., head_dim // 2, 2) real\n",
    "    #        -> flatten last two dims -> (..., head_dim) real\n",
    "    xq_out = torch.view_as_real(rotated_xq).flatten(3)\n",
    "    xk_out = torch.view_as_real(rotated_xk).flatten(3)\n",
    "\n",
    "    # 8. Cast back to the original input datatype (e.g., float16)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "# Calculate RoPE frequencies (precompute usually)\n",
    "# Note: RoPE is applied to head_dim, not hidden_size\n",
    "freqs_cis = simple_rope_calculation(head_dim, max_position_embeddings, base=rope_theta, device=hidden_states.device)\n",
    "print(f\"Calculated freqs_cis shape: {freqs_cis.shape}\") # (max_pos_emb, head_dim)\n",
    "\n",
    "# Apply RoPE\n",
    "# Note: RoPE is applied *before* repeating K/V for GQA\n",
    "query_states_rope, key_states_rope = apply_rotary_emb_torch(query_states, key_states, freqs_cis)\n",
    "\n",
    "print(\"\\nShapes after RoPE:\")\n",
    "print(f\"  query_states_rope: {query_states_rope.shape}\")\n",
    "print(f\"  key_states_rope: {key_states_rope.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Optional: QK Normalization\n",
    "#\n",
    "# Llama 4 sometimes includes an optional L2 normalization applied to Q and K *after* RoPE but *before* the attention score calculation. This is controlled by `config.use_qk_norm`.\n",
    "\n",
    "# %%\n",
    "# Llama4TextL2Norm implementation (simplified from original code)\n",
    "class SimpleL2Norm(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps  # Epsilon value to avoid division by zero during normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize along the last dimension (head_dim)\n",
    "        # This function normalizes the input tensor 'x' along its last dimension.\n",
    "        # It computes the L2 norm (Euclidean norm) of 'x' and scales 'x' by the inverse of this norm.\n",
    "        # The epsilon value is added to the denominator to ensure numerical stability and avoid division by zero.\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "if use_qk_norm:\n",
    "    qk_norm = SimpleL2Norm()\n",
    "    query_states_final = qk_norm(query_states_rope)\n",
    "    key_states_final = qk_norm(key_states_rope)\n",
    "    print(\"\\nApplied QK Norm\")\n",
    "else:\n",
    "    query_states_final = query_states_rope\n",
    "    key_states_final = key_states_rope\n",
    "    print(\"\\nSkipped QK Norm\")\n",
    "\n",
    "print(\"\\nShapes before attention score calculation:\")\n",
    "print(f\"  query_states_final: {query_states_final.shape}\")\n",
    "print(f\"  key_states_final: {key_states_final.shape}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4: Grouped-Query Attention (GQA) - Key/Value Repeating\n",
    "#\n",
    "# Since we have fewer K and V heads than Q heads (GQA), we need to \"repeat\" the K and V heads so that each Q head has a corresponding K and V to attend to. The `repeat_kv` function handles this.\n",
    "\n",
    "# %%\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Repeats Key/Value heads for GQA.\n",
    "    Input: (batch, num_key_value_heads, seqlen, head_dim)\n",
    "    Output: (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "# Repeat K and V heads\n",
    "key_states_repeated = repeat_kv(key_states_final, num_key_value_groups)\n",
    "value_states_repeated = repeat_kv(value_states, num_key_value_groups) # Use original value_states, RoPE/Norm not applied to V\n",
    "\n",
    "print(\"\\nShapes after repeating K/V for GQA:\")\n",
    "print(f\"  key_states_repeated: {key_states_repeated.shape}\")   # Should match Q heads dimension\n",
    "print(f\"  value_states_repeated: {value_states_repeated.shape}\") # Should match Q heads dimension\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5: Scaled Dot-Product Attention Calculation\n",
    "#\n",
    "# Now we perform the standard scaled dot-product attention:\n",
    "#\n",
    "# \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\]\n",
    "#\n",
    "# 1.  **Calculate Attention Scores:** Compute the dot product between Queries (Q) and Keys (K^T).\n",
    "# 2.  **Scale:** Scale the scores by \\( 1/\\sqrt{d_k} \\) (where \\(d_k\\) is `head_dim`). Llama 4 also has `attn_scale` and `attn_temperature_tuning` for layers without RoPE, but we'll omit those details here.\n",
    "# 3.  **Apply Mask:** Add the attention mask (`causal_mask`) to the scores. This prevents positions from attending to future positions (and optionally masks padding). Masked positions typically get a large negative value (like -inf).\n",
    "# 4.  **Softmax:** Apply softmax along the key dimension to get attention weights (probabilities).\n",
    "# 5.  **Dropout:** Apply dropout to the attention weights (optional, during training).\n",
    "# 6.  **Calculate Output:** Compute the weighted sum of Values (V) using the attention weights.\n",
    "\n",
    "# %%\n",
    "# 1. Calculate Attention Scores (Q @ K^T)\n",
    "# Q: (batch, num_attn_heads, seq_len, head_dim)\n",
    "# K: (batch, num_attn_heads, seq_len, head_dim) -> K^T: (batch, num_attn_heads, head_dim, seq_len)\n",
    "# Result: (batch, num_attn_heads, seq_len, seq_len)\n",
    "attn_weights = torch.matmul(query_states_final, key_states_repeated.transpose(2, 3))\n",
    "\n",
    "# 2. Scale\n",
    "scaling_factor = 1.0 / math.sqrt(head_dim)\n",
    "attn_weights = attn_weights * scaling_factor\n",
    "\n",
    "# 3. Apply Mask\n",
    "# Ensure mask shape is broadcastable: (batch, 1, seq_len, seq_len)\n",
    "if attention_mask is not None:\n",
    "    print(f\"\\nApplying attention mask with shape: {attention_mask.shape}\")\n",
    "    # Make sure mask covers the correct key length dimension\n",
    "    causal_mask = attention_mask[:, :, :, :key_states_repeated.shape[-2]] # slice mask's key dim\n",
    "    attn_weights = attn_weights + causal_mask\n",
    "else:\n",
    "     print(\"\\nNo attention mask applied.\")\n",
    "\n",
    "# 4. Softmax\n",
    "attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n",
    "\n",
    "# 5. Dropout (skipped for inference example)\n",
    "# attn_weights = nn.functional.dropout(attn_weights, p=attention_dropout, training=self.training)\n",
    "\n",
    "# 6. Calculate Output (Weighted Sum of Values)\n",
    "# attn_weights: (batch, num_attn_heads, seq_len, seq_len)\n",
    "# V: (batch, num_attn_heads, seq_len, head_dim)\n",
    "# Result: (batch, num_attn_heads, seq_len, head_dim)\n",
    "attn_output = torch.matmul(attn_weights, value_states_repeated)\n",
    "\n",
    "print(\"\\nAttention Calculation Shapes:\")\n",
    "print(f\"  attn_weights (raw scores): {attn_weights.shape}\")\n",
    "print(f\"  attn_weights (after softmax): {attn_weights.shape}\")\n",
    "print(f\"  attn_output: {attn_output.shape}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6: Reshape and Output Projection\n",
    "#\n",
    "# Finally, the attention output heads are concatenated and passed through a final linear layer (`o_proj`) to project them back to the `hidden_size`.\n",
    "\n",
    "# %%\n",
    "# Reshape attention output\n",
    "# (batch, num_attn_heads, seq_len, head_dim) -> (batch, seq_len, num_attn_heads, head_dim)\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "# -> (batch, seq_len, num_attn_heads * head_dim) = (batch, seq_len, hidden_size)\n",
    "attn_output = attn_output.view(batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# Apply output projection\n",
    "final_attn_output = o_proj(attn_output)\n",
    "\n",
    "print(\"\\nFinal Output Shapes:\")\n",
    "print(f\"  attn_output (reshaped): {attn_output.shape}\")\n",
    "print(f\"  final_attn_output: {final_attn_output.shape}\") # Should be (batch, seq_len, hidden_size)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7: Putting it Together (Simplified Llama4TextAttention Forward Pass)\n",
    "#\n",
    "# Let's combine the steps into a simplified forward function.\n",
    "\n",
    "# %%\n",
    "class SimplifiedLlama4Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.num_attention_heads = config['num_attention_heads']\n",
    "        self.num_key_value_heads = config['num_key_value_heads']\n",
    "        self.head_dim = self.hidden_size // self.num_attention_heads\n",
    "        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config['max_position_embeddings']\n",
    "        self.rope_theta = config['rope_theta']\n",
    "        self.attention_bias = config['attention_bias']\n",
    "        self.use_qk_norm = config['use_qk_norm']\n",
    "\n",
    "        if (self.head_dim * self.num_attention_heads) != self.hidden_size:\n",
    "            raise ValueError(\"hidden_size must be divisible by num_attention_heads\")\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_attention_heads * self.head_dim, bias=self.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.num_attention_heads * self.head_dim, self.hidden_size, bias=self.attention_bias)\n",
    "\n",
    "        self.freqs_cis = simple_rope_calculation(self.head_dim, self.max_position_embeddings, base=self.rope_theta)\n",
    "\n",
    "        if self.use_qk_norm:\n",
    "             self.qk_norm = SimpleL2Norm()\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, position_ids):\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "\n",
    "        # Projections\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # Reshape\n",
    "        query_states = query_states.view(batch_size, sequence_length, self.num_attention_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(batch_size, sequence_length, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(batch_size, sequence_length, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply RoPE\n",
    "        current_freqs_cis = self.freqs_cis.to(hidden_states.device) # Get precomputed freqs\n",
    "        query_states_rope, key_states_rope = apply_rotary_emb_torch(query_states, key_states, current_freqs_cis)\n",
    "\n",
    "        # Optional QK Norm\n",
    "        if self.use_qk_norm:\n",
    "             query_states_final = self.qk_norm(query_states_rope)\n",
    "             key_states_final = self.qk_norm(key_states_rope)\n",
    "        else:\n",
    "            query_states_final = query_states_rope\n",
    "            key_states_final = key_states_rope\n",
    "\n",
    "\n",
    "        # Repeat K/V for GQA\n",
    "        key_states_repeated = repeat_kv(key_states_final, self.num_key_value_groups)\n",
    "        value_states_repeated = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        # Attention Calculation\n",
    "        attn_weights = torch.matmul(query_states_final, key_states_repeated.transpose(2, 3))\n",
    "        scaling_factor = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn_weights = attn_weights * scaling_factor\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = attention_mask[:, :, :, :key_states_repeated.shape[-2]]\n",
    "            attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n",
    "        # Dropout would be here in training\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states_repeated)\n",
    "\n",
    "        # Reshape and Output Projection\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, sequence_length, self.hidden_size)\n",
    "        final_attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return final_attn_output, attn_weights # Return weights for inspection\n",
    "\n",
    "\n",
    "# Instantiate and run the simplified module\n",
    "config_dict = {\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_attention_heads': num_attention_heads,\n",
    "    'num_key_value_heads': num_key_value_heads,\n",
    "    'max_position_embeddings': max_position_embeddings,\n",
    "    'rope_theta': rope_theta,\n",
    "    'attention_bias': attention_bias,\n",
    "    'use_qk_norm': use_qk_norm,\n",
    "}\n",
    "\n",
    "simplified_attn_module = SimplifiedLlama4Attention(config_dict)\n",
    "\n",
    "# Run forward pass\n",
    "final_output_simplified, final_weights_simplified = simplified_attn_module(hidden_states, attention_mask, position_ids)\n",
    "\n",
    "print(\"\\nOutput shape from simplified module:\", final_output_simplified.shape)\n",
    "print(\"Attention weights shape from simplified module:\", final_weights_simplified.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
