{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b048f-a163-4b25-940f-1cf20ce72a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa6ce6-6bb8-4f56-a7cd-42f760f79f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=0):\n",
    "        super(CNNBlock, self).__init__()\n",
    "\n",
    "        self.seq_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6485da93-07cd-4e50-8bbd-a19bc1526c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlocks(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    n_conv (int): creates a block of n_conv convolutions\n",
    "    in_channels (int): number of in_channels of the first block's convolution\n",
    "    out_channels (int): number of out_channels of the first block's convolution\n",
    "    expand (bool) : if True after the first convolution of a blocl the number of channels doubles\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_conv,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 padding):\n",
    "        super(CNNBlocks, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_conv):\n",
    "\n",
    "            self.layers.append(CNNBlock(in_channels, out_channels, padding=padding))\n",
    "            # after each convolution we set (next) in_channel to (previous) out_channels\n",
    "            in_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42e356-2b11-4440-81ed-15188070050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    in_channels (int): number of in_channels of the first CNNBlocks\n",
    "    out_channels (int): number of out_channels of the first CNNBlocks\n",
    "    padding (int): padding applied in each convolution\n",
    "    downhill (int): number times a CNNBlocks + MaxPool2D it's applied.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 padding,\n",
    "                 downhill=4):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(downhill):\n",
    "            self.enc_layers += [\n",
    "                    CNNBlocks(n_conv=2, in_channels=in_channels, out_channels=out_channels, padding=padding),\n",
    "                    nn.MaxPool2d(2, 2)\n",
    "                ]\n",
    "\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2\n",
    "        # doubling the dept of the last CNN block\n",
    "        self.enc_layers.append(CNNBlocks(n_conv=2, in_channels=in_channels,\n",
    "                                         out_channels=out_channels, padding=padding))\n",
    "\n",
    "    def forward(self, x):\n",
    "        route_connection = []\n",
    "        for layer in self.enc_layers:\n",
    "            if isinstance(layer, CNNBlocks):\n",
    "                x = layer(x)\n",
    "                route_connection.append(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x, route_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149b14d-5ebb-4f34-b209-92cab3fdd958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    in_channels (int): number of in_channels of the first ConvTranspose2d\n",
    "    out_channels (int): number of out_channels of the first ConvTranspose2d\n",
    "    padding (int): padding applied in each convolution\n",
    "    uphill (int): number times a ConvTranspose2d + CNNBlocks it's applied.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 exit_channels,\n",
    "                 padding,\n",
    "                 uphill=4):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.exit_channels = exit_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(uphill):\n",
    "\n",
    "            self.layers += [\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "                CNNBlocks(n_conv=2, in_channels=in_channels,\n",
    "                          out_channels=out_channels, padding=padding),\n",
    "            ]\n",
    "            in_channels //= 2\n",
    "            out_channels //= 2\n",
    "\n",
    "        # cannot be a CNNBlock because it has ReLU incorpored\n",
    "        # cannot append nn.Sigmoid here because you should be later using\n",
    "        # BCELoss () which will trigger the amp error \"are unsafe to autocast\".\n",
    "        self.layers.append(\n",
    "            nn.Conv2d(in_channels, exit_channels, kernel_size=1, padding=padding),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, routes_connection):\n",
    "        # pop the last element of the list since\n",
    "        # it's not used for concatenation\n",
    "        routes_connection.pop(-1)\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, CNNBlocks):\n",
    "                # center_cropping the route tensor to make width and height match\n",
    "                routes_connection[-1] = center_crop(routes_connection[-1], x.shape[2])\n",
    "                # concatenating tensors channel-wise\n",
    "                x = torch.cat([x, routes_connection.pop(-1)], dim=1)\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642cacee-b73a-475a-91b2-58e6123bac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 first_out_channels,\n",
    "                 exit_channels,\n",
    "                 downhill,\n",
    "                 padding=0\n",
    "                 ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, first_out_channels, padding=padding, downhill=downhill)\n",
    "        self.decoder = Decoder(first_out_channels*(2**downhill), first_out_channels*(2**(downhill-1)),\n",
    "                               exit_channels, padding=padding, uphill=downhill)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_out, routes = self.encoder(x)\n",
    "        out = self.decoder(enc_out, routes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d25f0-59de-4747-8b07-001f2a3489fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "\n",
    "    img = np.array(Image.open(os.path.join(self.db_root_dir, self.img_list[idx])).convert(\"RGB\"), dtype=np.float32)\n",
    "    gt = np.array(Image.open(os.path.join(self.db_root_dir, self.labels[idx])).convert(\"L\"), dtype=np.float32)\n",
    "\n",
    "    gt = ((gt/np.max([gt.max(), 1e-8])) > 0.5).astype(np.float32)\n",
    "    #gt = gt.astype(np.bool).astype(np.float32)\n",
    "\n",
    "    if self.transform is not None:\n",
    "\n",
    "        augmentations = self.transform(image=img, mask=gt)\n",
    "        img = augmentations[\"image\"]\n",
    "        gt = augmentations[\"mask\"]\n",
    "\n",
    "    if self.pad_mirroring:\n",
    "        img = Pad(padding=self.pad_mirroring, padding_mode=\"reflect\")(img)\n",
    "\n",
    "    return img, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8f517-c516-4be9-90e2-e9116510ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    model = UNET(3, 64, 1, padding=0, downhill=4).to(DEVICE)\n",
    "    optim = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    if CHECKPOINT:\n",
    "        load_model_checkpoint(CHECKPOINT, model)\n",
    "        load_optim_checkpoint(CHECKPOINT, optim)\n",
    "    \n",
    "    train_loader, val_loader = get_loaders(db_root_dir=ROOT_DIR, batch_size=8, train_transform=train_transform,\n",
    "                                           val_transform=val_transforms, num_workers=4)\n",
    "    for epoch in range(10, EPOCHS):\n",
    "        print(f\"Training epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        train_loop(model=model, loader=train_loader, loss_fn=loss_fn, optim=optim, scaler=scaler, pos_weight=False)\n",
    "        \n",
    "        print(\"Computing dice_loss on val_loader...\")\n",
    "        \n",
    "        evalution_metrics(model, val_loader, loss_fn, device=DEVICE)\n",
    "\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optim.state_dict(),\n",
    "        }\n",
    "        \n",
    "        save_checkpoint(checkpoint, folder_path=SAVE_MODEL_PATH,\n",
    "                        filename=f\"checkpoint_epoch_{epoch+1}.pth.tar\")\n",
    "\n",
    "        save_images(model=model, loader=val_loader, folder=SAVE_IMAGES_PATH,\n",
    "                    epoch=epoch, device=DEVICE, num_images=10, pad_mirroring=PAD_MIRRORING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
