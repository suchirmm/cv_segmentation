{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e75536-c6bb-477c-ae07-42942d1f4ba0",
   "metadata": {},
   "source": [
    "# Interview Questions For Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f1a28-221c-43b0-a5fe-7b0c3d144e56",
   "metadata": {},
   "source": [
    "## 1. RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b69c98-35ac-48bf-959e-44aa46b0e871",
   "metadata": {},
   "source": [
    "### Q. What is RAG ? \n",
    "* Retrieval Augmented Generation\n",
    "* Technique to generate responses on data the model may not have been trained on\n",
    "* Leverages the power of in-context learning for LLMs\n",
    "* Relevant context is typically retrieved from vector database and model responses are generated based on that retrieved context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93d1e6-a174-4049-9828-7fe827a35c3a",
   "metadata": {},
   "source": [
    "### Q. What is the time complexity of RAG retrieval ?\n",
    "* O(n) for flat index search;\n",
    "* log(n) for IVF and HNSW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875aea5-66f5-4dfe-a30a-6d401e9af7f7",
   "metadata": {},
   "source": [
    "### Q. How does IVF work in RAG ?\n",
    "* Cluster the documents based on embeddings eg. k-means clustering\n",
    "* Find simliarity of query to cluster centroid\n",
    "* Search within the most similar centroid for most close document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca37f07-86cf-4fa6-8852-1037d74d8dbc",
   "metadata": {},
   "source": [
    "### Q. What is contextual precision which is often utilized in evaluating RAG systems/pipelines ?\n",
    "* The correctness of the retrievals with respect to the query\n",
    "* For k = 1 to n where n is int\n",
    "* The precision is calculated for all the values of k and the average is calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5928e1-f6cc-4544-95f5-a886196992b6",
   "metadata": {},
   "source": [
    "### Q. Which are the techniques that can reduce model hallucinations ? \n",
    "* Fine-tuning\n",
    "* Retrieval Augmented Generation\n",
    "* Few-shot and chain-of-thought prompting\n",
    "* Lowering the temperature hyperparameter\n",
    "* Training Data filtering and preprocessing \n",
    "* Careful system prompt design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f2de5-2f31-46a6-9322-2b7423619100",
   "metadata": {},
   "source": [
    "### Q. Which are techniques to find document similarity ?\n",
    "* Cosine Similarity \n",
    "* Jaccard distance\n",
    "* Euclidean Distance\n",
    "* Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770d98b-f581-43c6-a127-62e1c6dad4e3",
   "metadata": {},
   "source": [
    "###  Q. What are model hallucinations ?\n",
    "* Factually incorrect generations by the model\n",
    "* LLMs are autoregressive models and focus on next token predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a155a1da-b243-470d-8720-d04549c8499e",
   "metadata": {},
   "source": [
    "### Q. What are position embeddings as used in modern LLMs ? \n",
    "* Token wise embedding corresponding to position in the context\n",
    "* The transformer can understand the position of the token in the context\n",
    "* Otherwise attention mechanism will calculate same attention score for all tokens irrespective of position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fc723-3428-447b-8e3f-df34bca6d1e7",
   "metadata": {},
   "source": [
    "### Q. What is RAG re-ranking ?\n",
    "* A technique used to evaluate the evaluate the correctness of retrieval step\n",
    "* The similarity of the retrieved chunk and the query is calculated better than embedding similarity\n",
    "* Adds significant overhead and latency\n",
    "* Can be performed using Cross Encoder and ColBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70d3c5-33b4-4ac5-914a-d562bf270cdd",
   "metadata": {},
   "source": [
    "###  Q. Which is the more appropriate technique for million-scale documents vector Database in Retrieval Augmented Generation ?\n",
    "* IVF, HNSW as they have log(n) time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b528c-654a-4a6a-9654-16aa7946d94a",
   "metadata": {},
   "source": [
    "## 2. Neural Neural Networks - Transformers, Self Attention, LLMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab229298-abd2-46df-b98e-9db2c3587aee",
   "metadata": {},
   "source": [
    "###  Q. Why are skip connections used in deep neural networks ?\n",
    "* To avoid the vanishing gradient problem\n",
    "* Enables training of deeper networks with more parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68deae7d-77d2-42fe-8444-7f2dcc0276ec",
   "metadata": {},
   "source": [
    "### Q. Why has attention mechanism been used in NLP tasks?\n",
    "* RNNs utilized before struggled when the context was very long\n",
    "* Attention could capture dependencies between tokens\n",
    "* Attention enabled parallel training of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eeedde-d475-46e6-b34d-4acac52eae12",
   "metadata": {},
   "source": [
    "### Q. What are the drawbacks of attention mechanism used in Transformer architecture ?\n",
    "* n^2 computation time required for computing self-attention\n",
    "* Vast amount of training data required for training\n",
    "* Struggles with varied sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716dd4a6-6e66-439e-8da7-36a338678f81",
   "metadata": {},
   "source": [
    "### Q. In computing self attention in Transformers what are the 3 matrices that are used ? What is the scaling constant used in the Transformer model ?\n",
    "* Query, Key, Value - Q,K, V\n",
    "* the scaling value used is sqrt(dimension of embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa994e-5785-4a4a-8dcd-3a6207c829fd",
   "metadata": {},
   "source": [
    "### Q. Which normalization technique is used in Transformers ?\n",
    "* Layer normalization\n",
    "* Performs normalization based on the feature values\n",
    "* Normalizes features on forward pass and gradients on the backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799b59f-7f51-44c6-b0c0-a21ec196c8cb",
   "metadata": {},
   "source": [
    "### Q. Why do modern LLMs not use Dropout in their architecture ?\n",
    "* Lots of training data for pre-training and instruction tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05dad9a-5feb-448c-82fd-4421b93b6867",
   "metadata": {},
   "source": [
    "### Q. What is KV caching ?\n",
    "* A method to improve the inference time\n",
    "* Key, Value tokens are cached\n",
    "* Avoids repetitive computation of Query, Key computations\n",
    "* Increases the memory requirements marginally\n",
    "* Reduces inference time in some cases by 5 X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858248e-b18c-40b7-b6ef-d3e03d168aca",
   "metadata": {},
   "source": [
    "### Q. What is sliding attention window mechanism seen in modern LLM architectures like Mistral ?\n",
    "* Reduces the computation of attention to smaller context\n",
    "* Uses the concept of receptive field from CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6c5df-ad1e-4472-b97b-f145023c1d30",
   "metadata": {},
   "source": [
    "### Q. Why is RMS Normalization utilized in LLMs?\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9333d-d8db-421f-b7c7-caa71b4f2d66",
   "metadata": {},
   "source": [
    "### Q. How does the temperature hyperparameter affect a models outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d8885-6fc9-452a-a9c0-106efdd61327",
   "metadata": {},
   "source": [
    "### Q. Why are LLMS generally based on decoder-only architectures ? What is meant by the term auto-regressive models ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b88aa9-352b-408c-93e6-1164cd6dd616",
   "metadata": {},
   "source": [
    "### Q. What is the loss function used in LLMs ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bad81-789b-42c8-aa35-f44714524ecd",
   "metadata": {},
   "source": [
    "### Q. What is meant by quantization in context of modern Large Language Models ? How does it help in latency and also memory requirements ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b148b-f059-402d-a93d-1cfcdda22ae5",
   "metadata": {},
   "source": [
    "### Q. What is the pre-training step in LLMs ? Is it supervised, unsupervised or self-supervised training ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe1cb47-44ca-4884-847b-52c9d0a6e94c",
   "metadata": {},
   "source": [
    "### Q. What is Mixture of Experts(MOEs) in LLMs ? How does the gating function work in LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a09dfa5-fcf5-418d-9aa2-88cbb046ef4b",
   "metadata": {},
   "source": [
    "### Q. What is the full form of GPT in modern LLMs ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b3003-3ded-4c17-97d7-649d0fa4cfbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6eb66-61ff-4ca5-96b8-f1fb08538625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d3aae56-f884-48e5-a6d2-0ab69b6ac39b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7503d8-1766-43a4-a382-da01b8e118f2",
   "metadata": {},
   "source": [
    "### Q. What is the purpose of Normalization in Neural Networks ?\n",
    "* To stabilize training\n",
    "* Preventing covariate shift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d5eb6-fd2e-4faf-9fe2-928e73622393",
   "metadata": {},
   "source": [
    "### Q. What is the initialization techniques used when training neural networks ?\n",
    "* ReLU specific initialization\n",
    "* tanh, sigmoid specific initialization\n",
    "* Avoid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da677d-7a3b-4c01-b130-f1e28b024d84",
   "metadata": {},
   "source": [
    "### Q. What is the bias-variance trade off ?\n",
    "* To improve the generalization of models on real-world, unseen data\n",
    "* Bias is the performance on training data\n",
    "* Variance is the performance on test data\n",
    "* Overfit models will have low bias and high variance\n",
    "* Underfit models will have high bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd35ab-4b88-4399-97ef-cc2e112b3096",
   "metadata": {},
   "source": [
    "### Q. What are some strategies to reduce high bias / underfit in Neural networks ?\n",
    "* Longer training\n",
    "* More number of parameters\n",
    "* Change the optimization strategy - SGD, Adam, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b1036-0d0e-4a78-b184-0403dffe4e34",
   "metadata": {},
   "source": [
    "### Q. What are some strategies to reduce high variance / overfit in Neural Networks ?\n",
    "* L1, L2 regularization\n",
    "* Overfitting\n",
    "* Dropout (reduce the number of active parameters) in each layer\n",
    "* Do training on larger volume of data so that data is more diverse and reflects variability seen in real-world\n",
    "* Cross-validation on hold-out validation set\n",
    "* Early stopping\n",
    "* Weight decay - Reduces the updates to weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba128f-0278-4657-a0ca-a570d23e8824",
   "metadata": {},
   "source": [
    "### Q. Why is neural network weights not set to zero in training step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24200a75-cf32-480a-8dea-9b6b242c03e1",
   "metadata": {},
   "source": [
    "### Q. Explain the concept of transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e50421-3c4b-4a57-beb7-e95aa3aeb091",
   "metadata": {},
   "source": [
    "### Q. What is the purpose of Bias parameter in Neural networks ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f37f9a-e306-484b-a4aa-9ea575bd8b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68dcfd-5fde-4c05-afbb-95053a973de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fd9a5-1991-41d4-9bb0-8ee556accb41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6050035-e08e-431b-8fdf-ed6ef5308c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a633e-7c91-48f0-af3a-4674a93a22d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b92a9186-593c-498b-95d2-700fee6c16e7",
   "metadata": {},
   "source": [
    "## 4. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8bf35-2e70-4e11-a3f3-4724c6a459a3",
   "metadata": {},
   "source": [
    "### Q. How to handle imbalanced classification problems ?\n",
    "* Get more training data\n",
    "* Correct any bias, errors in sampling or measurement\n",
    "* Generate synthetic data\n",
    "* Oversample minority class\n",
    "* Undersample majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c59f1-0427-412f-b705-6386c29bb651",
   "metadata": {},
   "source": [
    "### Q. What are the assumptions for linear regression ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4a8e8-b804-4424-9358-4329e1cb6ba0",
   "metadata": {},
   "source": [
    "### Q. What are the assumptions for logisitic regression ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce369c0-4ee7-4343-b3bd-81231fb8f334",
   "metadata": {},
   "source": [
    "### Q. What is the loss function used in logsitic regression ?\n",
    "* BCE (Binary Cross Entropy)\n",
    "* Logistic regression performs binary classification\n",
    "* It finds best linear separator between target classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c595f588-3737-4e89-b7e2-ae87962277e3",
   "metadata": {},
   "source": [
    "### Q. What is the difference between bagging and boosting ?\n",
    "* Bagging takes predictions from different trees; uses mode for classification and mean for regression\n",
    "* In boosting - errors from previous trees are improved upon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb6986-790f-422b-86c1-e406771fd38f",
   "metadata": {},
   "source": [
    "### Q. What are the metrics used to decide decision tree splitting ?\n",
    "* Entropy\n",
    "* Gini impurity\n",
    "* Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a802c8a7-7226-4cc4-b865-719e7ac707ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b63b5b-cda6-46cb-8ef8-e6b7af08c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is the time complexity of RAG retrieval ? For O\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9785cd9-8036-44ae-a673-bf306afbf8cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62130b-4760-4881-b29a-7447f030a04a",
   "metadata": {},
   "source": [
    "<!-- Interview questions \n",
    " \n",
    "Which of the following is not a technique that can reduce model hallucinations ?\n",
    "\n",
    "Which of the following are techniques to find document similarity ?\n",
    "Principal Component Analysis\n",
    "Cosine Similarity \n",
    "Jaccard distance\n",
    "Euclidean Distance\n",
    "Clustering \n",
    "What is contextual recall which is often utilized in evaluating RAG systems/pipelines ?\n",
    "The correctness of the retrievals with respect to the query\n",
    "The correctness of the retrievals with respect to the generation\n",
    "The correctness of the retrievals with respect to the other retrievals\n",
    "The overlap of the generation with the ground truth response\n",
    "What are position embeddings as used in modern LLMs ? \n",
    "Word embeddings with random noise\n",
    "Token wise embedding corresponding to position in the context\n",
    "Indexing of the context\n",
    "What is the vanishing gradient problem in Deep Learning ? \n",
    "The gradient descent optimization is stuck in local minima\n",
    "Neural network gets a NAN value during backpropagation\n",
    "In deep neural networks the gradient is unable to propagate backwards to the initial layers\n",
    "What is the exploding gradient problem in Neural Networks ? \n",
    "Time taken explodes as Gradient descent takes too long due to indecipherable black box nature of neural networks   \n",
    "Gradient becomes too large or NAN values causing large corrections to the parameters and hence unstable training \n",
    "In deep neural networks the gradient is unable to propagate backwards to the initial layers\n",
    "Validation error shows a large divergence from Training Error\n",
    "Activation functions show unexpected behaviour\n",
    "What is the purpose of the activation function in neural networks ?\n",
    "Reduce the number of necessary computations from one layer to another\n",
    "Initializes the parameters of the model in the correct range so that model loss converges to global minimum\n",
    "Makes the model able to learn the linear mapping of the values to the target values\n",
    "Capture the non linear patterns of the data to train the model parameters\n",
    "\n",
    "\n",
    "\n",
    "What are skip connections useful for in Neural Networks ? \n",
    "To make deeper neural networks\n",
    "For reducing the computation complexity of neural network arithmetic\n",
    "Keeping parameters within a certain range\n",
    "Capturing non-linear patterns in the data \n",
    "Why are current generation SOTA LLM Models decoder only models ? \n",
    "Due to the task being of generating new tokens\n",
    "Decoder models have fewer parameters\n",
    "Decoder models have in built efficiencies due to cross-attention mechanism\n",
    "Where is Sigmoid Activation Function used commonly ? \n",
    "Regression tasks\n",
    "Reinforcement learning \n",
    "Classification tasks\n",
    "Feature normalization of the input layer\n",
    "Parameter initialization of the weights and biases\n",
    "\n",
    "What is the difference between ReLU and Leaky ReLU ? \n",
    "Leaky RELU adds a small value for negative values of f(x) hence it is differentiable everywhere\n",
    "Leaky RELU is faster than ReLU\n",
    "Leaky RELU has small gradients at extreme values\n",
    "Leaky RELU is asymptotic at large values of x \n",
    "What is the function of the optimiser in the Pytorch snippet below ? \n",
    "Prevents the vanishing gradient and exploding gradient problems\n",
    "Zeros out the gradient at each backpropagation step\n",
    "Finds the optimum parameters to reach global minimum of cost function in back propagation\n",
    "\n",
    "\n",
    "Why is Early stopping used ? \n",
    "To save computational resources when GPU resource is constrained or to let GPU servers cool down\n",
    "To prevent overfit \n",
    "For analyzing loss on various epochs\n",
    "The self attention mechanism used in Transformers has a time complexity of \n",
    "Log n\n",
    "n \n",
    "n2 \n",
    "n3\n",
    "What is jailbreaking in the context of LLMs ? \n",
    "Releasing models weights illegally  \n",
    "Ability of the model to express its repressed internal thoughts, conciousness\n",
    "Making the model behave in a non aligned manner and ignore safety instructions\n",
    "Unlocking generalisation capabilities of LLM beyond the initial training instructions or expected capacities of the model\n",
    "From the given options what is the best step for reducing underfit of models ? \n",
    "Longer training time by adding critical idle time between epochs for GPU cooling\n",
    "Reduce depth of neural network \n",
    "Reduce the number of parameters of the model\n",
    "Increasing the number of epochs\n",
    "What is the utility of BLEU, ROUGE, METEOR ?\n",
    "Classification tasks\n",
    "NLP tasks for neural network models\n",
    "Object Detection metrics\n",
    "\n",
    "\n",
    "What is the difference between Stemming and Lemmatization ?\n",
    "Stemming makes the  subword tokens while lemmatization makes the character level tokens\n",
    "Lemmatization preserves the meaning of the word after altering the token\n",
    "Stemming is more computational and time intensive than Lemmatization\n",
    "Lemmatization will produce more out of vocabulary tokens than stemming\n",
    "Subword tokenization is preferred in modern LLMs for which of the following reasons \n",
    "Balances vocabulary size \n",
    "Makes the vocabulary size the smallest possible \n",
    "Understands all the mappings between tokens \n",
    "In a Precision-Recall Curve what is the observed impact on Precision, Recall values as confidence threshold is increased\n",
    "No impact \n",
    "Precision will increase and Recall will decrease \n",
    "Precision will decrease and Recall will increase\n",
    "Few shot prompting is accurately described by which of the following statements \n",
    "Multiple examples given to the guide the output\n",
    "LLM correcting itself by fine-tuning itself on new data\n",
    "LLM is allowed to make errors to come up with final correct solution after many errors\n",
    "Context is reworded in different ways to guide the output \n",
    "What is the context window for an LLM ?\n",
    "Knowledge on particular topics stored in the weights of the model\n",
    "Number of tokens used in pre training \n",
    "Number of tokens on which Self Attention is applied in Sliding Window Attention\n",
    "Number of tokens that an LLM can take as an input\n",
    "What does the temperature hyperparameter that is seen in modern LLM hyperparameters effect ? \n",
    "The slowness and thoughtfulness of the model\n",
    "The probability distribution for sampling of the models output tokens\n",
    "The maximum number of tokens generated by the LLM\n",
    "The inference speed of tokens\n",
    "Why is Sigmoid Activation Function not used so much in modern LLMs and Deep Learning models ? \n",
    "Mathematical intuition is difficult to grasp\n",
    "For extreme values gradients are small so back propagation does not occur ideally to optimize the parameters of the Neural network \n",
    "It can only support classification tasks \n",
    "Not differentiable for all values of the function \n",
    "What is Mean Average Precision (mAP) for object detection tasks ? \n",
    "Average of precision for different values \n",
    "How is mAP calculated for Object Detection tasks ? \n",
    "Arrange the following in order of complexity and level of detail in Computer Vision ?\n",
    "Classification > Object Detection > Segmentation\n",
    "Segmentation > Classification > Object Detection\n",
    "What is Non Max Suppression used for in Object Detection ? \n",
    "Convolutions that reduce high pixel values from the feature map \n",
    "Removing highly overlapping bounding boxes and retaining only those predictions with the highest confidence \n",
    "Remove outliers of extremely high confidence from predictions \n",
    "<Some other>\n",
    "How does Non Max Suppression work in Object Detection ? \n",
    "Highest value Bounding Boxes based on confidence -> Remove high overlap -> \n",
    "Keeps only bounding boxes of a threshold \n",
    "Which of the following is not a regularisation method used in Neural networks training ? \n",
    "Dropout \n",
    "Batch Normalisation \n",
    "Higher number of epochs\n",
    "Early stopping \n",
    "Mini batch gradient descent \n",
    " Which of the following best describes an overfit model ? \n",
    "Poor performance on validation set\n",
    "Poor performance on training set\n",
    "Poor performance on test set \n",
    "Which of the following methods cannot be used to reduce the memory requirement for LLMs ? \n",
    "Knowledge Distillation \n",
    "Pruning \n",
    "Quantization \n",
    "Low Rank Adaptor \n",
    "Full precision training \n",
    "Which of the following describes precision accurately ? \n",
    "When given a regression task how many of the values of a particular predicted class are indeed correctly predicted\n",
    "When given a classification task how many of the values of a particular predicted class are indeed correctly predicted \n",
    "When given a classification task how high the confidence score is for the predicted values \n",
    "Which of the following statements describes frequency penalty most aptly ? \n",
    "It is a hyperparameter seen in modern LLMs that relates to tokens being generated out of turn\n",
    "It is a hyperparameter seen in modern LLMs that relates to tokens being generated beyond the maximum number of tokens\n",
    "It is a hyperparameter seen in modern LLMs that relates to tokens being repeated numerous times\n",
    "How does GRPO differ from other RL algorithms like PPO and DPO ? \n",
    "Samples are generated by the LLM and require less human feedback \n",
    "What is an aligned model ? \n",
    "One that gives honest, helpful and harmless responses \n",
    "The model that is finetuned for a particular domain \n",
    "Model that is quantized and has reduced memory requirements  \n",
    "What is perplexity when it comes to language models ?  \n",
    "The more surprised a model is when generating a new sequence of tokens\n",
    "The time complexity of Retrieval Augmented Generation\n",
    "What is zero-shot when it comes to LLMs ?\n",
    "Model hallucinations when it has not got diverse training data\n",
    "Prompting technique\n",
    "First stage of LLM training\n",
    "Autoregressive model generation task\n",
    "What are the data augmentation techniques suitable for NLP and language tasks ? \n",
    "Synthetic data generation by LLMs \n",
    "Smoothing and adding noise \n",
    "Shearing / Rotation\n",
    "Gray scaling \n",
    "Which of the following given below is incorrect about the kernel trick in Support Vector Machines ?\n",
    "To alter the distribution of the high dimensional data to make it linearly separable for the linear boundary to\n",
    "The model\n",
    "With regards to k-means clustering which of the following is False ?\n",
    "It is not suitable for high dimensionality dataset\n",
    "It always finds the most ideal clusters\n",
    "It forms circular clusters\n",
    "It is a supervised learning approach\n",
    "Elbow method can be used to find the ideal number of clusters\n",
    "What is correct regarding XG Boost ?\n",
    "It is an ensemble learning method only for classification tasks\n",
    "It is an ensemble learning method only for regression tasks\n",
    "It is an unsupervised method which requires no target variables\n",
    "XG Boost can be utilized with tree or linear models\n",
    "Why are metrics apart from accuracy required in imbalanced Classification problem statements ?\n",
    "Accuracy can be flawed when there is high class imbalance as naive predictions can predict majority class\n",
    "Accuracy is sufficient as it holistic and covers all situations comprehensively\n",
    "Accuracy becomes inconclusive in problems other than binary classification prediction tasks \n",
    "Accuracy can be flawed when there is high class imbalance as naive predictions can predict minority class\n",
    "Which is not an assumption for Linear Regression ?\n",
    "Residuals are normally distributed\n",
    "Independent variables are normally distributed\n",
    "Mean value of residuals is 0\n",
    "There is no significant correlation between independent variables\n",
    "There exists a linear relationship between the target variable and the independent variable\n",
    "Which of the following is not a technique for outlier removal ?\n",
    "Capping\n",
    "Deletion of sample\n",
    "Clustering\n",
    "Feature Encoding\n",
    "Which of the following is not a technique for improving performance on imbalanced classification tasks ?\n",
    "Upsampling\n",
    "Downsampling\n",
    "SMOTE and synthetic data generation\n",
    "Collecting more data \n",
    "Checking for measurement or sampling errors\n",
    "Converting to parquet format \n",
    "What is concept drift in ML models ?\n",
    "Change in the ML paradigms as technology evolves\n",
    "A phenomenon seen in ML model deployed in product where the relation between input feature and the target value changes\n",
    "Change in the  -->\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
